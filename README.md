# Sistema Multi-Agentes para Suporte da Hotmart ğŸ”¥

Sistema inteligente de atendimento ao cliente utilizando LLMs, RAG e Function Calling para responder perguntas sobre a Hotmart.

## ğŸ—ï¸ Arquitetura do Sistema

O sistema Ã© composto por 3 agentes principais:

* **Agente Central:** Gerencia a comunicaÃ§Ã£o e roteamento das conversas
* **Agente FAQ:** Busca informaÃ§Ãµes na base de conhecimento usando RAG
* **Agente Hotmart Journey:** Especialista em "ConheÃ§a a Hotmart Journey: Stars e Legacy" com acesso a dados personalizados de usuÃ¡rios

Este sistema foi desenvolvido para simular um atendimento inteligente multi-agente usando inteligÃªncia artificial generativa. Ele identifica o tipo de pergunta feita por um usuÃ¡rio e direciona a consulta para o agente mais adequado.

O fluxo geral segue estes passos:

<p align="left">
  <img src="https://raw.githubusercontent.com/vqrca/sistema-multi-agentes/main/Imagens/fluxo.png" width="70%"/>
</p>

1. UsuÃ¡rio envia uma pergunta via interface Streamlit.

2. Um Agente Central analisa a pergunta e faz uma classificaÃ§Ã£o:

    * 'FAQ': para dÃºvidas gerais sobre a Hotmart (ex: funcionalidades, pagamentos, configuraÃ§Ãµes)

    * 'JOURNEY': para perguntas sobre o programa Hotmart Journey: Stars e Legacy (ex: benefÃ­cios, faturamento, status do usuÃ¡rio)

3. A pergunta Ã© entÃ£o repassada para o agente adequado:

4. O Agente FAQ consulta a base vetorial FAISS com os artigos da FAQ e retorna uma resposta clara e objetiva.

5. O Agente Journey acessa dados fictÃ­cios de usuÃ¡rios (com base no ID) e retorna uma resposta especÃ­fica sobre o programa e seus benefÃ­cios.

6. A resposta Ã© exibida diretamente na interface para o usuÃ¡rio.


## ğŸ§° Tecnologias utilizadas

Este projeto foi desenvolvido com as seguintes tecnologias:

| Tecnologia | DescriÃ§Ã£o |
|------------|-----------|
| **LLM via Groq**<br>`llama-3.3-70b-versatile` | Modelo de linguagem de alta performance utilizado para interpretar perguntas, classificar rotas e gerar respostas |
| **HuggingFace Embeddings**<br>`intfloat/multilingual-e5-large` | Modelo de embeddings multilÃ­ngue utilizado para converter perguntas e documentos em vetores semÃ¢nticos |
| **LlamaIndex** | Framework para conectar LLMs a fontes externas de dados e orquestrar buscas contextuais (RAG) |
| **FAISS** | Biblioteca de busca vetorial rÃ¡pida e eficiente usada para construir a base de conhecimento da FAQ |
| **CrewAI** | Sistema de orquestraÃ§Ã£o multi-agente para executar tarefas distribuÃ­das entre agentes inteligentes |
| **Streamlit** | Ferramenta para criaÃ§Ã£o de interfaces web rÃ¡pidas e interativas, usada na prototipaÃ§Ã£o do sistema |
| **Docker & Docker Compose** | Utilizados para empacotar, isolar e facilitar a execuÃ§Ã£o local do ambiente de testes |


## ğŸ“ Estrutura do Projeto

```bash
projeto/
â”œâ”€â”€ Dockerfile             # Define o ambiente Docker com Streamlit
â”œâ”€â”€ docker-compose.yaml    # Orquestra o container Docker e expÃµe a aplicaÃ§Ã£o
â”œâ”€â”€ .env                   # Armazena a chave de API
â”œâ”€â”€ exemplos/
â”‚   â””â”€â”€ testes.sh          # Script para simular perguntas de FAQ e Journey via cURL
â”œâ”€â”€ faiss_index/           # Arquivos da base vetorial de conhecimento (gerados com FAISS)
â”œâ”€â”€ agentes.py             # Define o agente central, agente de FAQ e agente do programa Hotmart Journey
â”œâ”€â”€ App.py                 # Interface com Streamlit para testar o protÃ³tipo via web
â”œâ”€â”€ vector_index.py        # Carregamento dos Ã­ndices da base de conhecimento com embeddings do Hugging Face
â”œâ”€â”€ ferramentas.py         # Ferramentas utilizadas pelos agentes (ex: busca FAISS, API mockada)
â”œâ”€â”€ main_agent_router.py   # LÃ³gica de roteamento da pergunta, definindo qual agente serÃ¡ acionado
â”œâ”€â”€ requirements.txt       # DependÃªncias necessÃ¡rias para rodar o projeto
â””â”€â”€ README.md              # DocumentaÃ§Ã£o geral do projeto
```

## ğŸš€ Teste online via Streamlit Cloud

VocÃª pode acessar a aplicaÃ§Ã£o diretamente, sem precisar instalar nada localmente.

ğŸ‘‰ [Clique aqui para acessar no Streamlit Cloud](https://sistema-multi-agentes.streamlit.app)

> A aplicaÃ§Ã£o estÃ¡ hospedada e pronta para testes com perguntas gerais e personalizadas usando IDs.

---

## ğŸ” Como executar localmente com Docker

Este projeto foi desenvolvido para ser facilmente executado usando Docker e Docker Compose. Siga os passos abaixo para rodar a aplicaÃ§Ã£o localmente:

### âœ… PrÃ©-requisitos

- Docker instalado: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)
- Docker Compose (jÃ¡ incluÃ­do nas versÃµes atuais do Docker)

---

### ğŸš€ Passo a passo

#### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/vqrca/sistema-multi-agentes.git
cd sistema-multi-agentes
```

#### 2. Construa e execute a aplicaÃ§Ã£o
```bash
docker-compose up --build
```
A aplicaÃ§Ã£o serÃ¡ executada na porta 8501. Acesse no navegador:
http://localhost:8501

## ğŸ§ª Testes e Reprodutibilidade

Para facilitar os testes da aplicaÃ§Ã£o, este repositÃ³rio inclui exemplos de perguntas que podem ser usadas tanto para dÃºvidas gerais quanto para consultas personalizadas com ID de usuÃ¡rio.

### ğŸ“‚ Arquivos disponÃ­veis

- `testes/perguntas_exemplo.txt`: contÃ©m um conjunto de perguntas prontas para copiar e colar na interface.
- `testes/abrir_app.sh`: script shell para abrir automaticamente a aplicaÃ§Ã£o no navegador local (`http://localhost:8501`).

### â–¶ï¸ Como testar

1. ApÃ³s executar o projeto com Docker (veja seÃ§Ã£o anterior), rode no terminal:

```bash
bash testes/abrir_app.sh
```

2. Em seguida, visualize as perguntas de teste com:
```bash
cat testes/perguntas_exemplo.txt
```

## ğŸ“„ RelatÃ³rio Final
Para complementar a entrega, foi elaborado um relatÃ³rio detalhado com a estratÃ©gia de validaÃ§Ã£o, mÃ©tricas utilizadas e uma anÃ¡lise da qualidade das respostas geradas pelos agentes.

ğŸ‘‰ [Clique aqui para acessar o relatÃ³rio final](https://github.com/vqrca/sistema-multi-agentes/blob/main/relatorio_final.md)

O relatÃ³rio aborda como o sistema lida com perguntas ambÃ­guas, como os agentes sÃ£o avaliados com base em relevÃ¢ncia e precisÃ£o, e apresenta sugestÃµes para evoluÃ§Ã£o futura do protÃ³tipo.

Este projeto demonstra como agentes inteligentes, combinados com LLMs e tÃ©cnicas de RAG, podem ser aplicados para melhorar o suporte ao cliente de forma personalizada e escalÃ¡vel. A arquitetura modular permite fÃ¡cil expansÃ£o para novos domÃ­nios e fluxos de atendimento.

Mais do que um protÃ³tipo funcional, esta soluÃ§Ã£o propÃµe uma abordagem realista e reprodutÃ­vel para construir sistemas conversacionais baseados em conhecimento, com atenÃ§Ã£o Ã  qualidade, relevÃ¢ncia e capacidade de adaptaÃ§Ã£o.



